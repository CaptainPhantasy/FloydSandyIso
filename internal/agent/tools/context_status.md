# Context Status Tool

**Name:** `context_status`

## Description

Returns current context window usage statistics. Use this to monitor your context consumption and know when to be more concise or when you have room for detailed responses.

**Key Feature:** This tool is cache-safe. It does not inject dynamic content into the prompt (which would break caching). Instead, it reads token usage from the session state on-demand.

## Parameters

None required.

## Response Fields

| Field | Type | Description |
|-------|------|-------------|
| `prompt_tokens` | int64 | Total tokens in the conversation prompt |
| `completion_tokens` | int64 | Total tokens generated by the assistant |
| `cache_read_tokens` | int64 | Tokens read from cache (not re-processed) |
| `effective_tokens` | int64 | Actual token count (prompt + completion - cached) |
| `context_window` | int64 | Maximum context window for the model |
| `percent_used` | float64 | Percentage of context window used |
| `remaining_tokens` | int64 | Tokens remaining before hitting limit |
| `should_summarize` | bool | True if approaching limit (80%+) |
| `session_id` | string | Current session identifier |

## Example Response

```
Context: 23.5% used (47,000/200,000 tokens). 153,000 remaining.
Cached: 12,000 tokens | Prompt: 35,000 | Completion: 12,000
```

## When to Use

- Before large operations that may generate extensive output
- When unsure how much context headroom remains
- Before spawning sub-agents or lengthy analysis
- When approaching conversation limits (80%+ warning triggers)

## Cache Safety

This tool is designed to preserve prompt caching:
- Does not modify any cached prompt content
- Reads from session state (updated after each API response)
- Returns dynamic data only in tool response (not in prompt)
